# Basic agent evaluation configuration for quick testing
benchmarks:
  - name: agentbench
    enabled: true
    environments: ["os", "db"]  # Start with simpler environments
    num_samples: 50
  
  - name: toolbench
    enabled: true
    categories: ["search", "calculator"]
    num_samples: 100
  
  - name: swe_bench
    enabled: true
    difficulty: ["easy"]
    num_samples: 25

evaluation:
  parallel_workers: 2
  timeout_per_task: 300  # 5 minutes per task
  max_retries: 2
  save_trajectories: true

agent:
  # Agent configuration will be loaded from separate agent config
  timeout: 60  # seconds per action
  max_iterations: 20
  enable_logging: true

safety:
  enable_content_filter: true
  enable_safety_classifier: false  # Disable for basic eval
  max_harmful_outputs: 0

reporting:
  formats: ["json", "html"]
  include_trajectories: true
  include_examples: true
  num_examples: 10
  output_dir: "./results"
  
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "agent_evaluation.log"